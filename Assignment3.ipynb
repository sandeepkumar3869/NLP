{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk,string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.Tokenize and tag the following sentence: They wind back the clock, while we chase after the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'wind', 'back', 'clock', 'chase', 'wind']\n"
     ]
    }
   ],
   "source": [
    "# Original text to be processed\n",
    "text = \"They wind back the clock, while we chase after the wind.\"\n",
    "\n",
    "# Convert the text to lowercase for uniformity in processing\n",
    "lower_text = text.lower()\n",
    "\n",
    "# Define a set of English stopwords using NLTK\n",
    "stop_worrd = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "# print(stop_worrd)  # This would print the stopwords if needed\n",
    "\n",
    "# Remove punctuation from the text by using the 'translate' method\n",
    "# text.maketrans creates a mapping to remove punctuation characters\n",
    "text_prep = text.translate(text.maketrans(\"\", \"\", string.punctuation))\n",
    "# print(text_prep)  # This would print the text without punctuation\n",
    "\n",
    "# Initialize an empty list to store words after removing stopwords\n",
    "free_0f_stopword = []\n",
    "\n",
    "# Tokenize the text (split it into individual words)\n",
    "toknized_word = word_tokenize(text_prep)\n",
    "# print(toknized_word)  # This would print the tokenized words\n",
    "\n",
    "# Loop through each word in the tokenized list\n",
    "for i in toknized_word:\n",
    "    # Check if the word is not a stopword\n",
    "    if i not in stop_worrd:\n",
    "        # Append the word to the list if it's not a stopword\n",
    "        free_0f_stopword.append(i)\n",
    "\n",
    "# Print the list of words that are free of stopwords\n",
    "print(free_0f_stopword)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('They', 'PRP'), ('wind', 'VBP'), ('back', 'RB'), ('the', 'DT'), ('clock', 'NN'), ('while', 'IN'), ('we', 'PRP'), ('chase', 'VBP'), ('after', 'IN'), ('the', 'DT'), ('wind', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the POS-tagged tokens\n",
    "pos_tagg = []\n",
    "\n",
    "# Perform POS tagging on the tokenized words and append the result to the 'pos_tagg' list\n",
    "pos_tagg.append(nltk.pos_tag(toknized_word))\n",
    "\n",
    "# Print the POS-tagged tokens\n",
    "print(pos_tagg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Use the Penn Treebank tagset to tag each word in the following sentences from Damon Runyon’s short stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('nice', 'JJ'), ('night', 'NN')]\n",
      "[('This', 'DT'), ('crap', 'NN'), ('game', 'NN'), ('is', 'VBZ'), ('over', 'RP'), ('a', 'DT'), ('garage', 'NN'), ('in', 'IN'), ('Fiftysecond', 'NNP'), ('Street', 'NNP')]\n",
      "[('…Nobody', 'NN'), ('ever', 'RB'), ('takes', 'VBZ'), ('the', 'DT'), ('newspapers', 'NNS'), ('she', 'PRP'), ('sells', 'VBZ')]\n",
      "[('…I', 'NN'), ('am', 'VBP'), ('sitting', 'VBG'), ('in', 'IN'), ('Mindy', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('restaurant', 'JJ'), ('putting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('gefillte', 'NN'), ('fish', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('a', 'DT'), ('dish', 'JJ'), ('I', 'PRP'), ('am', 'VBP'), ('very', 'RB'), ('fond', 'NN'), ('of', 'IN')]\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# List of sentences to be processed\n",
    "list_of_sentence = [\n",
    "    \"i. It is a nice night.\",\n",
    "    \"ii. This crap game is over a garage in Fifty-second Street... 3\",\n",
    "    \"iii. …Nobody ever takes the newspapers she sells ...\",\n",
    "    \"iv. …I am sitting in Mindy’s restaurant putting on the gefillte fish, which is a dish I am very fond of, ...\",\n",
    "    \"v. The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "# Empty list to store preprocessed sentences\n",
    "list_of_sentence_preprocessing = []\n",
    "\n",
    "# Loop through each sentence in the list\n",
    "for i in list_of_sentence:\n",
    "    # Check if the sentence contains a period ('.')\n",
    "    if '.' in i:\n",
    "        # Split the sentence by the first occurrence of a period and strip any leading/trailing spaces\n",
    "        i = i.split('.', 1)[1].strip()\n",
    "        # Remove punctuation and numbers from the sentence\n",
    "        i = i.translate(i.maketrans(\"\", \"\", string.punctuation + \"0123456789\")).strip()\n",
    "        # Tokenize the sentence and append to the preprocessed list\n",
    "        list_of_sentence_preprocessing.append(word_tokenize(i))\n",
    "    else:\n",
    "        # If no period is found, just remove punctuation and numbers\n",
    "        i = i.translate(i.maketrans(\"\", \"\", string.punctuation + \"0123456789\")).strip()\n",
    "        # Tokenize the sentence and append to the preprocessed list\n",
    "        list_of_sentence_preprocessing.append(word_tokenize(i))\n",
    "\n",
    "# The next block (currently commented) would handle removal of stopwords from the tokenized sentences\n",
    "# free_of_stopwordd = []\n",
    "# for each word in the first tokenized sentence, check if it's not a stopword, then append\n",
    "# for j in list_of_sentence_preprocessing[0]:\n",
    "#    if j not in stop_worrd:\n",
    "#       free_of_stopwordd.append(j)\n",
    "\n",
    "# The same logic is applied to all tokenized sentences (currently commented)\n",
    "# free_of_stopwordd = []\n",
    "# for i in range(len(list_of_sentence_preprocessing)):\n",
    "#    for j in list_of_sentence_preprocessing[i]:\n",
    "#       if j not in stop_worrd:\n",
    "#          free_0f_stopwordd.append(j)\n",
    "\n",
    "# Now we print the POS tags (part-of-speech tagging) for each preprocessed sentence\n",
    "list_of_sentence_preprocessing_pos_tagged = []\n",
    "\n",
    "# Loop through each preprocessed sentence\n",
    "for i in list_of_sentence_preprocessing:\n",
    "    # Get the POS tags for the tokenized sentence\n",
    "    posstag = nltk.pos_tag(i)\n",
    "    # Append the POS-tagged sentence to the list\n",
    "    list_of_sentence_preprocessing_pos_tagged.append(posstag)\n",
    "    # Print the POS-tagged sentence\n",
    "    print(posstag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Build an NER System\n",
    "\n",
    "i. Collect list of sentences containing named entities (e.g., names of people, places, organizations).\n",
    "\n",
    "ii. Define a function to tokenize and POS tag the collected list of sentences\n",
    "\n",
    "iii. Define a function built-in NER recognizer to identify specific named entities. (Use NLTK’s built-in named entity chunker (ne_chunk)\n",
    "to recognize named entities like people, organizations, and locations.)\n",
    "\n",
    "iv. Evaluate your NER system:\n",
    "Create a list of sentence containing instances of similar named entities.\n",
    "Call the above functions to check if your system correctly identified the names.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sandeepkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sandeepkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/sandeepkumar/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/sandeepkumar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sandeepkumar/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: wertyuikl;\n",
      "Named Entities: (S wertyuikl/NN)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import string\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def ner_system(prompt):\n",
    "    new_prompt = []\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    # Tokenize the input into words\n",
    "    words = word_tokenize(prompt)\n",
    "\n",
    "    for i in words:\n",
    "        if i.lower() not in stopwords:\n",
    "            # Clean up the word by removing punctuation and numbers\n",
    "            i_cleaned = i.translate(str.maketrans(\"\", \"\", string.punctuation + \"0123456789\")).strip()\n",
    "\n",
    "            if i_cleaned:\n",
    "                new_prompt.append(i_cleaned)\n",
    "\n",
    "    # POS tagging for the cleaned tokens\n",
    "    pos_tags = pos_tag(new_prompt)\n",
    "    \n",
    "    # Named Entity Recognition (NER)\n",
    "    ner_result = ne_chunk(pos_tags)\n",
    "\n",
    "    # Print sentence and its named entities\n",
    "    print(f\"Sentence: {prompt}\")\n",
    "    print(f\"Named Entities: {ner_result}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Input from user\n",
    "prompt = input(\"Enter your prompt: \")\n",
    "\n",
    "# Call the NER system\n",
    "ner_system(prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
